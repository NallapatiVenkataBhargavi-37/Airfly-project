{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMiCfeHi6Y9t8G9gwLzBWt0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["ML MODELS"],"metadata":{"id":"VZaMQVag6R1F"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","from sklearn.utils import class_weight\n","import tensorflow as tf\n","from tensorflow.keras import layers, models, callbacks\n","import joblib"],"metadata":{"id":"xoK9k6z-6apl","executionInfo":{"status":"ok","timestamp":1757078748441,"user_tz":-330,"elapsed":8306,"user":{"displayName":"Venkata Bhargavi Nallapati","userId":"11019158521973124949"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["CSV_PATH = \"cleaned_flights.csv\"\n","RANDOM_SEED = 42\n","BATCH_SIZE = 1024\n","EPOCHS = 30\n","VALIDATION_SPLIT = 0.1\n","\n","np.random.seed(RANDOM_SEED)\n","tf.random.set_seed(RANDOM_SEED)"],"metadata":{"id":"qwgytz4HB0ZM","executionInfo":{"status":"ok","timestamp":1757078750744,"user_tz":-330,"elapsed":19,"user":{"displayName":"Venkata Bhargavi Nallapati","userId":"11019158521973124949"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Load data\n","# ============================\n","df = pd.read_csv(CSV_PATH)\n","\n","# ============================\n","# Targets\n","# ============================\n","target_reg = next((c for c in [\"DEP_DELAY\",\"DEP_DELAY_NEW\",\"ARR_DELAY\"] if c in df.columns), None)\n","target_clf = next((c for c in [\"CANCELLED\",\"CANCELLED_IND\"] if c in df.columns), None)\n","if not target_reg or not target_clf:\n","    raise ValueError(\"Required target columns missing\")\n","\n","# ============================\n","# Features\n","# ============================\n","candidate_features = [\"AIRLINE\",\"ORIGIN\",\"DEST\",\"CRS_DEP_TIME_HOUR\",\"CRS_ARR_TIME_HOUR\",\n","                      \"Month\",\"DayOfWeek\",\"IsWeekend\",\"DISTANCE\",\"FL_NUMBER\",\"Route\"]\n","cols_lower = {c.lower(): c for c in df.columns}\n","features = [cols_lower.get(f.lower(), f) for f in candidate_features if f in df.columns or f.lower() in cols_lower]\n","\n","# Create Route if missing\n","if \"Route\" not in features and \"ORIGIN\" in df.columns and \"DEST\" in df.columns:\n","    df[\"Route\"] = df[\"ORIGIN\"].astype(str) + \"_\" + df[\"DEST\"].astype(str)\n","    features.append(\"Route\")"],"metadata":{"id":"HJXA0wAEB2tA","executionInfo":{"status":"ok","timestamp":1757078753466,"user_tz":-330,"elapsed":270,"user":{"displayName":"Venkata Bhargavi Nallapati","userId":"11019158521973124949"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["\n","# ============================\n","# Prepare X, y\n","# ============================\n","cat_feats = [f for f in features if df[f].dtype == \"object\"]\n","num_feats = [f for f in features if f not in cat_feats]\n","\n","df[cat_feats] = df[cat_feats].fillna(\"Unknown\")\n","y_reg = df[target_reg].fillna(0).clip(0).astype(float)\n","y_clf = df[target_clf].fillna(0).astype(int)\n","\n","X = pd.concat([\n","    df[num_feats].fillna(0).astype(float),\n","    pd.get_dummies(df[cat_feats].astype(str), drop_first=True) if cat_feats else pd.DataFrame(index=df.index)\n","], axis=1)\n"],"metadata":{"id":"vJ_kqZQpB_gs","executionInfo":{"status":"ok","timestamp":1757078756587,"user_tz":-330,"elapsed":217,"user":{"displayName":"Venkata Bhargavi Nallapati","userId":"11019158521973124949"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# ============================\n","# Train/test split\n","# ============================\n","if \"FL_DATE\" in df.columns:\n","    df[\"FL_DATE_dt\"] = pd.to_datetime(df[\"FL_DATE\"], errors=\"coerce\")\n","    df_sorted_idx = df.sort_values(\"FL_DATE_dt\").index\n","    split_idx = int(len(df_sorted_idx) * 0.8)\n","    train_idx, test_idx = df_sorted_idx[:split_idx], df_sorted_idx[split_idx:]\n","    X_train, X_test = X.loc[train_idx], X.loc[test_idx]\n","    y_train_reg, y_test_reg = y_reg.loc[train_idx], y_reg.loc[test_idx]\n","    y_train_clf, y_test_clf = y_clf.loc[train_idx], y_clf.loc[test_idx]\n","else:\n","    X_train, X_test, y_train_reg, y_test_reg, y_train_clf, y_test_clf = train_test_split(\n","        X, y_reg, y_clf, test_size=0.2, random_state=RANDOM_SEED\n","    )\n"],"metadata":{"id":"zq4WrZgtCCL6","executionInfo":{"status":"ok","timestamp":1757078760623,"user_tz":-330,"elapsed":286,"user":{"displayName":"Venkata Bhargavi Nallapati","userId":"11019158521973124949"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\n","# ============================\n","# Scale numeric features\n","# ============================\n","scaler = StandardScaler()\n","if num_feats:\n","    X_train[num_feats] = scaler.fit_transform(X_train[num_feats])\n","    X_test[num_feats] = scaler.transform(X_test[num_feats])\n","\n","# ============================\n","# Build models\n","# ============================\n","def build_regressor(input_dim):\n","    inp = layers.Input(shape=(input_dim,))\n","    x = layers.Dense(256, activation=\"relu\")(inp)\n","    x = layers.Dropout(0.2)(x)\n","    x = layers.Dense(128, activation=\"relu\")(x)\n","    x = layers.Dropout(0.15)(x)\n","    x = layers.Dense(64, activation=\"relu\")(x)\n","    out = layers.Dense(1, activation=\"linear\")(x)\n","    model = models.Model(inp, out)\n","    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n","    return model"],"metadata":{"id":"KM6QeCXlCF0_","executionInfo":{"status":"ok","timestamp":1757078763384,"user_tz":-330,"elapsed":63,"user":{"displayName":"Venkata Bhargavi Nallapati","userId":"11019158521973124949"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\n","def build_classifier(input_dim):\n","    inp = layers.Input(shape=(input_dim,))\n","    x = layers.Dense(256, activation=\"relu\")(inp)\n","    x = layers.Dropout(0.2)(x)\n","    x = layers.Dense(128, activation=\"relu\")(x)\n","    x = layers.Dropout(0.15)(x)\n","    x = layers.Dense(64, activation=\"relu\")(x)\n","    out = layers.Dense(1, activation=\"sigmoid\")(x)\n","    model = models.Model(inp, out)\n","    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"binary_accuracy\", tf.keras.metrics.AUC(name=\"auc\")])\n","    return model\n","\n","input_dim = X_train.shape[1]\n"],"metadata":{"id":"nqb0kVQxCJQs","executionInfo":{"status":"ok","timestamp":1757078767561,"user_tz":-330,"elapsed":19,"user":{"displayName":"Venkata Bhargavi Nallapati","userId":"11019158521973124949"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# ============================\n","# Regression (save as .h5, fixed)\n","# ============================\n","reg_model = build_regressor(input_dim)\n","reg_callbacks = [\n","    callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True),\n","    callbacks.ModelCheckpoint(\"delay_regressor.h5\", save_best_only=True)  # .h5 auto = HDF5 format\n","]\n","reg_model.fit(\n","    X_train, y_train_reg,\n","    validation_split=VALIDATION_SPLIT,\n","    epochs=EPOCHS,\n","    batch_size=BATCH_SIZE,\n","    callbacks=reg_callbacks,\n","    verbose=2\n",")\n","\n","# Predictions and metrics (compute RMSE manually)\n","y_pred_reg = reg_model.predict(X_test, batch_size=BATCH_SIZE).flatten()\n","mse = mean_squared_error(y_test_reg, y_pred_reg)\n","rmse = np.sqrt(mse)\n","mae = mean_absolute_error(y_test_reg, y_pred_reg)\n","r2 = r2_score(y_test_reg, y_pred_reg)\n","\n","print(f\"[Regression] MAE: {mae:.3f}, RMSE: {rmse:.3f}, R2: {r2:.3f}\")\n"],"metadata":{"id":"MSoNlf1HCLiT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757079126036,"user_tz":-330,"elapsed":28056,"user":{"displayName":"Venkata Bhargavi Nallapati","userId":"11019158521973124949"}},"outputId":"06f433da-daf4-41da-e4c8-f5d23f38684d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["9/9 - 6s - 690ms/step - loss: 2012.8074 - mae: 11.8889 - val_loss: 3531.7546 - val_mae: 16.0253\n","Epoch 2/30\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["9/9 - 5s - 543ms/step - loss: 1978.2733 - mae: 12.4291 - val_loss: 3448.1440 - val_mae: 16.7004\n","Epoch 3/30\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["9/9 - 1s - 159ms/step - loss: 1904.7917 - mae: 14.3953 - val_loss: 3297.0520 - val_mae: 19.2786\n","Epoch 4/30\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["9/9 - 1s - 146ms/step - loss: 1853.7703 - mae: 18.4510 - val_loss: 3242.2856 - val_mae: 21.5016\n","Epoch 5/30\n","9/9 - 1s - 147ms/step - loss: 1826.6141 - mae: 18.1655 - val_loss: 3262.5554 - val_mae: 20.0022\n","Epoch 6/30\n","9/9 - 3s - 278ms/step - loss: 1801.0557 - mae: 16.8782 - val_loss: 3248.7905 - val_mae: 20.3825\n","Epoch 7/30\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["9/9 - 1s - 148ms/step - loss: 1766.5382 - mae: 17.5090 - val_loss: 3229.8726 - val_mae: 21.3498\n","Epoch 8/30\n","9/9 - 2s - 171ms/step - loss: 1728.2485 - mae: 17.5614 - val_loss: 3234.6072 - val_mae: 21.2820\n","Epoch 9/30\n","9/9 - 3s - 300ms/step - loss: 1678.0627 - mae: 16.7240 - val_loss: 3242.3499 - val_mae: 21.4552\n","Epoch 10/30\n","9/9 - 2s - 238ms/step - loss: 1615.0557 - mae: 16.1313 - val_loss: 3259.7776 - val_mae: 21.7691\n","Epoch 11/30\n","9/9 - 1s - 153ms/step - loss: 1543.1633 - mae: 15.2084 - val_loss: 3292.1013 - val_mae: 21.8710\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n","[Regression] MAE: 20.187, RMSE: 46.899, R2: 0.012\n"]}]},{"cell_type":"code","source":["\n","# ============================\n","# Classification\n","# ============================\n","cw = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train_clf), y=y_train_clf)\n","class_weights = {i: cw[i] for i in range(len(cw))}\n","\n","clf_model = build_classifier(input_dim)\n","clf_callbacks = [\n","    callbacks.EarlyStopping(monitor=\"val_auc\", patience=4, restore_best_weights=True),\n","    callbacks.ModelCheckpoint(\"cancel_classifier.h5\", monitor=\"val_auc\", save_best_only=True, mode=\"max\")\n","]\n","clf_model.fit(X_train, y_train_clf, validation_split=VALIDATION_SPLIT, epochs=EPOCHS,\n","              batch_size=BATCH_SIZE, class_weight=class_weights, callbacks=clf_callbacks, verbose=2)\n","\n","y_proba = clf_model.predict(X_test, batch_size=BATCH_SIZE).flatten()\n","y_pred = (y_proba >= 0.5).astype(int)\n","print(f\"[Classification] AUC: {roc_auc_score(y_test_clf,y_proba):.4f}, \"\n","      f\"Acc: {accuracy_score(y_test_clf,y_pred):.4f}, \"\n","      f\"Prec: {precision_score(y_test_clf,y_pred,zero_division=0):.4f}, \"\n","      f\"Rec: {recall_score(y_test_clf,y_pred,zero_division=0):.4f}, \"\n","      f\"F1: {f1_score(y_test_clf,y_pred,zero_division=0):.4f}\")\n"],"metadata":{"id":"e7DnzwrPCPSe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757078986113,"user_tz":-330,"elapsed":14727,"user":{"displayName":"Venkata Bhargavi Nallapati","userId":"11019158521973124949"}},"outputId":"0ce3677d-7b2d-4c60-e83d-24cfb976e362"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["9/9 - 5s - 537ms/step - auc: 0.5568 - binary_accuracy: 0.4029 - loss: 0.7051 - val_auc: 0.5487 - val_binary_accuracy: 0.8593 - val_loss: 0.6591\n","Epoch 2/30\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["9/9 - 1s - 151ms/step - auc: 0.7464 - binary_accuracy: 0.5543 - loss: 0.6746 - val_auc: 0.5770 - val_binary_accuracy: 0.8955 - val_loss: 0.5821\n","Epoch 3/30\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["9/9 - 2s - 270ms/step - auc: 0.8063 - binary_accuracy: 0.6553 - loss: 0.6086 - val_auc: 0.5947 - val_binary_accuracy: 0.9538 - val_loss: 0.3973\n","Epoch 4/30\n","9/9 - 1s - 140ms/step - auc: 0.8779 - binary_accuracy: 0.7471 - loss: 0.4951 - val_auc: 0.5903 - val_binary_accuracy: 0.9548 - val_loss: 0.2683\n","Epoch 5/30\n","9/9 - 1s - 134ms/step - auc: 0.9533 - binary_accuracy: 0.8571 - loss: 0.3389 - val_auc: 0.5697 - val_binary_accuracy: 0.9166 - val_loss: 0.2380\n","Epoch 6/30\n","9/9 - 1s - 135ms/step - auc: 0.9844 - binary_accuracy: 0.9238 - loss: 0.1881 - val_auc: 0.5590 - val_binary_accuracy: 0.9256 - val_loss: 0.1908\n","Epoch 7/30\n","9/9 - 1s - 134ms/step - auc: 0.9918 - binary_accuracy: 0.9551 - loss: 0.1020 - val_auc: 0.5521 - val_binary_accuracy: 0.9568 - val_loss: 0.1502\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7cdfc6a47b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m2/3\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step "]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:6 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7cdfc6a47b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n","[Classification] AUC: 0.4672, Acc: 0.6785, Prec: 0.0140, Rec: 0.3056, F1: 0.0268\n"]}]},{"cell_type":"code","source":["\n","# ============================\n","# Save scaler & features\n","# ============================\n","joblib.dump(scaler, \"scaler.joblib\")\n","joblib.dump({\"num_feats\": num_feats, \"cat_feats\": cat_feats, \"features\": X.columns.tolist()}, \"feature_meta.joblib\")\n","\n","print(\"Saved models: delay_regressor.h5, cancel_classifier.h5, scaler.joblib, feature_meta.joblib\")"],"metadata":{"id":"QvosI8MNCTQf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757079137851,"user_tz":-330,"elapsed":72,"user":{"displayName":"Venkata Bhargavi Nallapati","userId":"11019158521973124949"}},"outputId":"78eb20d3-1c61-4ec9-de6c-631f067d1805"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved models: delay_regressor.h5, cancel_classifier.h5, scaler.joblib, feature_meta.joblib\n"]}]}]}